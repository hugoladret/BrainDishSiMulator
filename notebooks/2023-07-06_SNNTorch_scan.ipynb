{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import functional as SF\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm \n",
    "import psweep as ps # <--\n",
    "\n",
    "import warnings # highly illegal move to make pandas compliant\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# And now that we have a dataloader and a working network, we can sweep for a few parameters\n",
    "### Meta parameters would be lr, beta, threshold\n",
    "### Network parameters in a simple FF MLP would be depth, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_id = 9501 # experiment ID\n",
    "chip_session = 0 # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 10 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "len_trial = 100 # how long in bins is a trial (so in ms it's len_trial*binsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8c8b35",
   "metadata": {},
   "source": [
    "# Using our brand new dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61b42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 525.87it/s]\n",
      "c:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/utils\\utils_tensor.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(binned_spikes)\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 711.50it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 733.59it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 637.06it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 656.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "# Find maximum time across all spike lists\n",
    "max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "# Create binned spikes tensor for each region\n",
    "sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "\n",
    "# Verifying that the tensor are binary files\n",
    "utils_tensor.check_binary(sensory_spikes_binned, \"sensory_spikes_binned\")\n",
    "utils_tensor.check_binary(up1_spikes_binned, \"up1_spikes_binned\")\n",
    "utils_tensor.check_binary(down1_spikes_binned, \"down1_spikes_binned\")\n",
    "utils_tensor.check_binary(up2_spikes_binned, \"up2_spikes_binned\")\n",
    "utils_tensor.check_binary(down2_spikes_binned, \"down2_spikes_binned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# A bit of preprocessing to get a nice PyTorch friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing events\n",
    "# Process everything so its nice and milliseconds\n",
    "events[0]['event'] = 'motor layout: 0' # change the being game to motor layout for convenience \n",
    "for event in events:\n",
    "    event['norm_timestamp'] /= fs  # fs to seconds \n",
    "    event['norm_timestamp'] *= 1000  # seconds to ms\n",
    "    \n",
    "event_types = ['ball missed', 'ball bounce', 'ball return', 'motor layout: 0'] # these are all the labels\n",
    "labels = torch.tensor(utils_tensor.events_to_bins(events, event_types, 10, max_time_ms))\n",
    "assert labels.shape[-1] == sensory_spikes_binned.shape[-1] # make sure the labels and the data are the same length\n",
    "\n",
    "transformed_data, transformed_labels = utils_tensor.transform_data(labels, sensory_spikes_binned, len_trial) # change dataformat\n",
    "assert transformed_data.shape[1] == transformed_labels.shape[0] # make sure the labels and the data have the same trials\n",
    "\n",
    "# Create Dataset\n",
    "dataset = utils_tensor.CustomDataset(transformed_data, transformed_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82758fbe",
   "metadata": {},
   "source": [
    "# Now we define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59782b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, beta=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = len(num_hiddens)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(num_inputs if i==0 else num_hiddens[i-1], num_hiddens[i]) for i in range(self.num_layers)])\n",
    "        self.lifs = nn.ModuleList([snn.Leaky(beta=beta) for _ in range(self.num_layers)])\n",
    "        self.fc_out = nn.Linear(num_hiddens[-1], num_outputs)\n",
    "        self.lif_out = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mems = [lif.init_leaky() for lif in self.lifs]\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(x.size(1)):\n",
    "            cur = x[:,step]\n",
    "            for i in range(self.num_layers):\n",
    "                cur = self.fcs[i](cur)\n",
    "                spk, mems[i] = self.lifs[i](cur, mems[i])\n",
    "                cur = spk\n",
    "\n",
    "            out_spk, out_mem = self.lif_out(self.fc_out(cur), self.lif_out.init_leaky())\n",
    "            spk_rec.append(out_spk)\n",
    "            mem_rec.append(out_mem)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fdde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dishnet(params) :\n",
    "    # Unpack the parameters from the dict \n",
    "    num_inputs = params['num_inputs']\n",
    "    num_hiddens = params['num_hiddens']  # Changed to num_hiddens\n",
    "    num_outputs = params['num_outputs']\n",
    "    beta = params['beta']\n",
    "    num_epochs = params['num_epochs']\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    dishnet = Net(num_inputs = num_inputs, num_hiddens = num_hiddens, num_outputs=num_outputs, beta = beta).to(device)  # Changed to num_hiddens\n",
    "\n",
    "    loss_fn = SF.ce_count_loss()\n",
    "    optimizer = torch.optim.Adam(dishnet.parameters(), lr=lr, betas=(0.9, 0.999)) \n",
    "\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch = iter(data_loader)\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            dishnet.train()\n",
    "            spk_rec, _ = dishnet(data)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter == 100:\n",
    "                break\n",
    "            \n",
    "    print('\\n')\n",
    "    return loss_hist[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0707de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:55:49.067250252          32   0.9  0.001          50        [50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:56:45.058360338          32   0.9  0.001          50       [100]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:57:48.400750399          32   0.9  0.001          50       [150]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:58:53.943826675          32   0.9  0.001          50    [50, 50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:19<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:00:13.731577396          32   0.9  0.001          50  [100, 100]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:01:41.113162756          32   0.9  0.001          50  [150, 150]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:34<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs   num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:03:15.404095650          32   0.9  0.001          50  [50, 50, 50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:51<00:48,  2.02s/it]"
     ]
    }
   ],
   "source": [
    "# Define the lists of values for each parameter you want to sweep over\n",
    "num_inputs = ps.plist(\"num_inputs\", [transformed_data.shape[-1]])\n",
    "\n",
    "# Make multiple possible size for the input layers\n",
    "size_hiddens = np.linspace(50, 1000, 3, dtype=int)\n",
    "# Generate all possible combinations for 1, 2, and 3 layers.\n",
    "hidden_layer_sizes_1 = [[size] for size in size_hiddens]\n",
    "hidden_layer_sizes_2 = [[size1, size2] for size1 in size_hiddens for size2 in size_hiddens]\n",
    "hidden_layer_sizes_3 = [[size1, size2, size3] for size1 in size_hiddens for size2 in size_hiddens for size3 in size_hiddens]\n",
    "\n",
    "num_hiddens = ps.plist(\"num_hiddens\", hidden_layer_sizes_1 + hidden_layer_sizes_2 + hidden_layer_sizes_3)\n",
    "\n",
    "num_outputs = ps.plist(\"num_outputs\", [len(event_types)])\n",
    "beta = ps.plist(\"beta\", [0.9])\n",
    "num_epochs = ps.plist(\"num_epochs\", [50])\n",
    "lr = ps.plist(\"lr\", [1e-3])\n",
    "batch_size = ps.plist(\"batch_size\", [32])\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = ps.pgrid((num_inputs, num_hiddens, num_outputs, beta, num_epochs, lr, batch_size))\n",
    "\n",
    "# Define a function to run one instance of the experiment\n",
    "def run_experiment(params):\n",
    "    return {'loss': train_dishnet(params)}\n",
    "\n",
    "# Run the parameter sweep\n",
    "results = ps.run_local(run_experiment, param_grid, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cc819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num_inputs  num_hidden  num_outputs  beta  num_epochs     lr  batch_size  \\\n",
      "3          500         NaN            4   0.9          50  0.001          32   \n",
      "8          500         NaN            4   0.9          50  0.001          32   \n",
      "5          500         NaN            4   0.9          50  0.001          32   \n",
      "2          500       150.0            4   0.9          50  0.001          32   \n",
      "11         500         NaN            4   0.9          50  0.001          32   \n",
      "\n",
      "        loss  _pset_runtime      num_hiddens  \n",
      "3   0.018619      51.647575             [50]  \n",
      "8   0.102545      90.708937       [150, 150]  \n",
      "5   0.209881      60.721089            [150]  \n",
      "2   0.229214      57.529644              NaN  \n",
      "11  0.346576     128.648592  [150, 150, 150]  \n"
     ]
    }
   ],
   "source": [
    "# print the 5 rows with the lowest loss, with only the parameters and the loss\n",
    "lowest_losses = results.sort_values(by=['loss']).head(5)\n",
    "lowest_losses = lowest_losses.drop(columns=['_pset_hash', '_run_id', '_pset_id', '_calc_dir', '_time_utc', '_pset_seq', '_run_seq'])\n",
    "print(lowest_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
