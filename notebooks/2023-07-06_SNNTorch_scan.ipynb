{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import functional as SF\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm \n",
    "import psweep as ps # <--\n",
    "\n",
    "import warnings # highly illegal move to make pandas compliant\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# And now that we have a dataloader and a working network, we can sweep for a few parameters\n",
    "### Meta parameters would be lr, beta, threshold\n",
    "### Network parameters in a simple FF MLP would be depth, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_id = 9501 # experiment ID\n",
    "chip_session = 0 # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 10 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "len_trial = 100 # how long in bins is a trial (so in ms it's len_trial*binsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8c8b35",
   "metadata": {},
   "source": [
    "# Using our brand new dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61b42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 525.87it/s]\n",
      "c:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/utils\\utils_tensor.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(binned_spikes)\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 711.50it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 733.59it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 637.06it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 656.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "# Find maximum time across all spike lists\n",
    "max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "# Create binned spikes tensor for each region\n",
    "sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "\n",
    "# Verifying that the tensor are binary files\n",
    "utils_tensor.check_binary(sensory_spikes_binned, \"sensory_spikes_binned\")\n",
    "utils_tensor.check_binary(up1_spikes_binned, \"up1_spikes_binned\")\n",
    "utils_tensor.check_binary(down1_spikes_binned, \"down1_spikes_binned\")\n",
    "utils_tensor.check_binary(up2_spikes_binned, \"up2_spikes_binned\")\n",
    "utils_tensor.check_binary(down2_spikes_binned, \"down2_spikes_binned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# A bit of preprocessing to get a nice PyTorch friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing events\n",
    "# Process everything so its nice and milliseconds\n",
    "events[0]['event'] = 'motor layout: 0' # change the being game to motor layout for convenience \n",
    "for event in events:\n",
    "    event['norm_timestamp'] /= fs  # fs to seconds \n",
    "    event['norm_timestamp'] *= 1000  # seconds to ms\n",
    "    \n",
    "event_types = ['ball missed', 'ball bounce', 'ball return', 'motor layout: 0'] # these are all the labels\n",
    "labels = torch.tensor(utils_tensor.events_to_bins(events, event_types, 10, max_time_ms))\n",
    "assert labels.shape[-1] == sensory_spikes_binned.shape[-1] # make sure the labels and the data are the same length\n",
    "\n",
    "transformed_data, transformed_labels = utils_tensor.transform_data(labels, sensory_spikes_binned, len_trial) # change dataformat\n",
    "assert transformed_data.shape[1] == transformed_labels.shape[0] # make sure the labels and the data have the same trials\n",
    "\n",
    "# Create Dataset\n",
    "dataset = utils_tensor.CustomDataset(transformed_data, transformed_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82758fbe",
   "metadata": {},
   "source": [
    "# Now we define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59782b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, beta=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = len(num_hiddens)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(num_inputs if i==0 else num_hiddens[i-1], num_hiddens[i]) for i in range(self.num_layers)])\n",
    "        self.lifs = nn.ModuleList([snn.Leaky(beta=beta) for _ in range(self.num_layers)])\n",
    "        self.fc_out = nn.Linear(num_hiddens[-1], num_outputs)\n",
    "        self.lif_out = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mems = [lif.init_leaky() for lif in self.lifs]\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(x.size(1)):\n",
    "            cur = x[:,step]\n",
    "            for i in range(self.num_layers):\n",
    "                cur = self.fcs[i](cur)\n",
    "                spk, mems[i] = self.lifs[i](cur, mems[i])\n",
    "                cur = spk\n",
    "\n",
    "            out_spk, out_mem = self.lif_out(self.fc_out(cur), self.lif_out.init_leaky())\n",
    "            spk_rec.append(out_spk)\n",
    "            mem_rec.append(out_mem)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fdde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dishnet(params) :\n",
    "    # Unpack the parameters from the dict \n",
    "    num_inputs = params['num_inputs']\n",
    "    num_hiddens = params['num_hiddens']  # Changed to num_hiddens\n",
    "    num_outputs = params['num_outputs']\n",
    "    beta = params['beta']\n",
    "    num_epochs = params['num_epochs']\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * 0.8)  # 80% for training\n",
    "    test_size = dataset_size - train_size  # 20% for testing\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    dishnet = Net(num_inputs = num_inputs, num_hidden = num_hiddens, num_outputs=num_outputs, beta = beta).to(device)  # Changed to num_hiddens\n",
    "\n",
    "    loss_fn = SF.ce_count_loss()\n",
    "    optimizer = torch.optim.Adam(dishnet.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch = iter(train_loader)\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            dishnet.train()\n",
    "            spk_rec, _ = dishnet(data)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter == 100:\n",
    "                break\n",
    "        \n",
    "        # Testing phase\n",
    "        dishnet.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad(): \n",
    "            for data, targets in test_loader:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                spk_rec, _ = dishnet(data)\n",
    "\n",
    "                # Compute loss value\n",
    "                test_loss_val = loss_fn(spk_rec, targets)\n",
    "                \n",
    "                # Store test loss history\n",
    "                test_loss_hist.append(test_loss_val.item())\n",
    "                \n",
    "    print('\\n')\n",
    "    return {'train_loss': loss_hist, 'test_loss': test_loss_hist}  # Return the last train and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0707de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:55:49.067250252          32   0.9  0.001          50        [50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:56:45.058360338          32   0.9  0.001          50       [100]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:57:48.400750399          32   0.9  0.001          50       [150]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 21:58:53.943826675          32   0.9  0.001          50    [50, 50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:19<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:00:13.731577396          32   0.9  0.001          50  [100, 100]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:01:41.113162756          32   0.9  0.001          50  [150, 150]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:34<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs   num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:03:15.404095650          32   0.9  0.001          50  [50, 50, 50]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:42<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs      num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:04:57.897975445          32   0.9  0.001          50  [100, 100, 100]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs      num_hiddens  num_inputs  num_outputs\n",
      "2023-07-06 22:06:52.456018209          32   0.9  0.001          50  [150, 150, 150]         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:12<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the lists of values for each parameter you want to sweep over\n",
    "num_inputs = ps.plist(\"num_inputs\", [transformed_data.shape[-1]])\n",
    "\n",
    "# Make multiple possible size for the input layers\n",
    "size_hiddens = np.linspace(50, 1000, 3, dtype=int)\n",
    "# Generate all possible combinations for 1, 2, and 3 layers.\n",
    "hidden_layer_sizes_1 = [[size] for size in size_hiddens]\n",
    "hidden_layer_sizes_2 = [[size1, size2] for size1 in size_hiddens for size2 in size_hiddens]\n",
    "hidden_layer_sizes_3 = [[size1, size2, size3] for size1 in size_hiddens for size2 in size_hiddens for size3 in size_hiddens]\n",
    "\n",
    "num_hiddens = ps.plist(\"num_hiddens\", hidden_layer_sizes_1 + hidden_layer_sizes_2 + hidden_layer_sizes_3)\n",
    "\n",
    "num_outputs = ps.plist(\"num_outputs\", [len(event_types)])\n",
    "beta = ps.plist(\"beta\", [0.8, 0.9, 0.95])\n",
    "num_epochs = ps.plist(\"num_epochs\", [10, 50])\n",
    "lr = ps.plist(\"lr\", [1e-1, 1e-2, 1e-3])\n",
    "batch_size = ps.plist(\"batch_size\", [32])\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = ps.pgrid((num_inputs, num_hiddens, num_outputs, beta, num_epochs, lr, batch_size))\n",
    "\n",
    "# Define a function to run one instance of the experiment\n",
    "def run_experiment(params):\n",
    "    train_loss, test_loss = train_dishnet(params)\n",
    "    return {'train_loss': train_loss, 'test_loss': test_loss}\n",
    "\n",
    "# Run the parameter sweep\n",
    "results = ps.run_local(run_experiment, param_grid, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15cc819",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test_loss'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-06_SNNTorch_scan.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06_SNNTorch_scan.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m test_loss_list[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06_SNNTorch_scan.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Apply the function to each row in 'test_loss' column, sort the DataFrame by the final test loss\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06_SNNTorch_scan.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mfinal_test_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39;49m\u001b[39mtest_loss\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(final_test_loss)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06_SNNTorch_scan.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results_sorted \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfinal_test_loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06_SNNTorch_scan.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Select the top 5 models\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test_loss'"
     ]
    }
   ],
   "source": [
    "# Define a function to return the last test loss from each row\n",
    "def final_test_loss(test_loss_list):\n",
    "    return test_loss_list[-1]\n",
    "\n",
    "# Apply the function to each row in 'test_loss' column, sort the DataFrame by the final test loss\n",
    "results['final_test_loss'] = results['test_loss'].apply(final_test_loss)\n",
    "results_sorted = results.sort_values(by=['final_test_loss'])\n",
    "\n",
    "# Select the top 5 models\n",
    "top_5_models = results_sorted.head(5)\n",
    "\n",
    "# Plot the loss history for the top 5 models\n",
    "for i, row in top_5_models.iterrows():\n",
    "    plt.plot(row['test_loss'], label=f\"Model {i}\")\n",
    "\n",
    "plt.title('Test Loss Curves for the Top 5 Models')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
