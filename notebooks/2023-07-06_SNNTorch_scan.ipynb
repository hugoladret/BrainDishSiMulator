{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import functional as SF\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm \n",
    "import psweep as ps # <--\n",
    "\n",
    "import warnings # highly illegal move to make pandas compliant\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# And now that we have a dataloader and a working network, we can sweep for a few parameters\n",
    "### Meta parameters would be lr, beta, threshold\n",
    "### Network parameters in a simple FF MLP would be depth, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_id = 9501 # experiment ID\n",
    "chip_session = 0 # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 10 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "len_trial = 100 # how long in bins is a trial (so in ms it's len_trial*binsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8c8b35",
   "metadata": {},
   "source": [
    "# Using our brand new dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61b42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 523.05it/s]\n",
      "c:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/utils\\utils_tensor.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(binned_spikes)\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 734.27it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 759.29it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 689.70it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 710.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "# Find maximum time across all spike lists\n",
    "max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "# Create binned spikes tensor for each region\n",
    "sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "\n",
    "# Verifying that the tensor are binary files\n",
    "utils_tensor.check_binary(sensory_spikes_binned, \"sensory_spikes_binned\")\n",
    "utils_tensor.check_binary(up1_spikes_binned, \"up1_spikes_binned\")\n",
    "utils_tensor.check_binary(down1_spikes_binned, \"down1_spikes_binned\")\n",
    "utils_tensor.check_binary(up2_spikes_binned, \"up2_spikes_binned\")\n",
    "utils_tensor.check_binary(down2_spikes_binned, \"down2_spikes_binned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# A bit of preprocessing to get a nice PyTorch friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing events\n",
    "# Process everything so its nice and milliseconds\n",
    "events[0]['event'] = 'motor layout: 0' # change the being game to motor layout for convenience \n",
    "for event in events:\n",
    "    event['norm_timestamp'] /= fs  # fs to seconds \n",
    "    event['norm_timestamp'] *= 1000  # seconds to ms\n",
    "    \n",
    "event_types = ['ball missed', 'ball bounce', 'ball return', 'motor layout: 0'] # these are all the labels\n",
    "labels = torch.tensor(utils_tensor.events_to_bins(events, event_types, 10, max_time_ms))\n",
    "assert labels.shape[-1] == sensory_spikes_binned.shape[-1] # make sure the labels and the data are the same length\n",
    "\n",
    "transformed_data, transformed_labels = utils_tensor.transform_data(labels, sensory_spikes_binned, len_trial) # change dataformat\n",
    "assert transformed_data.shape[1] == transformed_labels.shape[0] # make sure the labels and the data have the same trials\n",
    "\n",
    "# Create Dataset\n",
    "dataset = utils_tensor.CustomDataset(transformed_data, transformed_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82758fbe",
   "metadata": {},
   "source": [
    "# Now we define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59782b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, beta=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(x.size(1)):\n",
    "            cur1 = self.fc1(x[:,step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fdde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dishnet(params) :\n",
    "    # Unpack the parameters from the dict \n",
    "    num_inputs = params['num_inputs']\n",
    "    num_hidden = params['num_hidden']\n",
    "    num_outputs = params['num_outputs']\n",
    "    beta = params['beta']\n",
    "    num_epochs = params['num_epochs']\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    dishnet = Net(num_inputs = num_inputs, num_hidden = num_hidden, num_outputs=num_outputs, beta = beta).to(device)\n",
    "\n",
    "    loss_fn = SF.ce_count_loss()\n",
    "    optimizer = torch.optim.Adam(dishnet.parameters(), lr=lr, betas=(0.9, 0.999)) # future Hugo : these are optimizer's beta, not the SNN's, dont be stupid\n",
    "\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch = iter(data_loader)\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            dishnet.train()\n",
    "            spk_rec, _ = dishnet(data)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter == 100:\n",
    "                break\n",
    "            \n",
    "    print('\\n')\n",
    "    return loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a1662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:30:01.849459410          32   0.9  0.001          50          50         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:50<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:30:52.633007050          32   0.9  0.001          50         100         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:31:45.305738211          32   0.9  0.001          50         150         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:57<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the lists of values for each parameter you want to sweep over\n",
    "num_inputs = ps.plist(\"num_inputs\", [transformed_data.shape[-1]])\n",
    "num_hidden = ps.plist(\"num_hidden\", [50, 100, 150])\n",
    "num_outputs = ps.plist(\"num_outputs\", [len(event_types)])\n",
    "beta = ps.plist(\"beta\", [0.9])\n",
    "num_epochs = ps.plist(\"num_epochs\", [50])\n",
    "lr = ps.plist(\"lr\", [1e-3])\n",
    "batch_size = ps.plist(\"batch_size\", [32])\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = ps.pgrid((num_inputs, num_hidden, num_outputs, beta, num_epochs, lr, batch_size))\n",
    "\n",
    "# Define a function to run one instance of the experiment\n",
    "def run_experiment(params):\n",
    "    return {'loss': train_dishnet(params)}\n",
    "\n",
    "# Run the parameter sweep\n",
    "results = ps.run_local(run_experiment, param_grid, verbose = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
