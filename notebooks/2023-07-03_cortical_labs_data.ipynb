{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9323efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats, optimize, interpolate\n",
    "from scipy.stats import alexandergovern, f_oneway\n",
    "\n",
    "import warnings # highly illegal move to make pandas compliant\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21583ab5",
   "metadata": {},
   "source": [
    "# Exploring the data from Cortical Lab\n",
    "### Everything is stocked in .h5 hierarchical files, that contain both spiketimes and electrode/channel information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7590ab",
   "metadata": {},
   "source": [
    "## Exploring the .h5 file structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd73ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assay\n",
      "Group\n",
      "-\n",
      "assay/inputs\n",
      "Group\n",
      "-\n",
      "mapping\n",
      "Dataset (900,) [('channel', '<i4'), ('electrode', '<i4'), ('x', '<f8'), ('y', '<f8')]\n",
      "-\n",
      "message_0\n",
      "Dataset (1,) |S12\n",
      "-\n",
      "proc0\n",
      "Group\n",
      "-\n",
      "proc0/spikeTimes\n",
      "Dataset (221562,) [('frame_no', '<u8'), ('channel', '<u4'), ('amplitude', '<f4')]\n",
      "-\n",
      "settings\n",
      "Group\n",
      "-\n",
      "settings/gain\n",
      "Dataset (1,) float64\n",
      "-\n",
      "settings/hpf\n",
      "Dataset (1,) float64\n",
      "-\n",
      "settings/lsb\n",
      "Dataset (1,) float64\n",
      "-\n",
      "sig\n",
      "Dataset (1028, 0) uint16\n",
      "-\n",
      "time\n",
      "Dataset (1,) |S12\n",
      "-\n",
      "version\n",
      "Dataset (1,) |S12\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "def print_structure(name, obj):\n",
    "    \"\"\"Prints the structure of the H5 file.\n",
    "\n",
    "    Args:\n",
    "    name : str\n",
    "        Name of the object.\n",
    "    obj : H5 object\n",
    "        The object itself.\n",
    "    \"\"\"\n",
    "    print(name)\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print('Dataset', obj.shape, obj.dtype)\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print('Group')\n",
    "    print('-')\n",
    "def explore_h5_file(filename):\n",
    "    \"\"\"Explore the structure of an H5 file.\n",
    "\n",
    "    Args:\n",
    "    filename : str\n",
    "        Path to the H5 file.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as file:\n",
    "        file.visititems(print_structure)\n",
    "\n",
    "# Use the function\n",
    "explore_h5_file('../data/cortical_labs_data/11614.2021-06-02.0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c82964",
   "metadata": {},
   "source": [
    "## Let's define some helper function to handle their H5 data (adapted from the code provided by Forough and Moein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0436f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_maxwell_h5(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts frame, channel, amplitude, electrode id, x, y, chip_id, date, and session from MAxwell H5 files.\n",
    "\n",
    "    Args:\n",
    "        data_path: directory of the spikeword (.npy) files\n",
    "\n",
    "    Returns:\n",
    "        data: dataframe including all of the information above about each H5 file\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    for filename in tqdm(os.listdir(data_path), desc = 'Loading data...', total = len(os.listdir(data_path))):\n",
    "        if filename.endswith('.h5'):\n",
    "\n",
    "            full_filename = os.path.join(data_path, filename)\n",
    "\n",
    "            with h5py.File(full_filename, 'r') as f:\n",
    "                proc0 = f['proc0']\n",
    "                spike_times = np.array(proc0['spikeTimes'])\n",
    "\n",
    "                if len(spike_times) > 0:\n",
    "                    file_data = h5_to_pd(full_filename)\n",
    "\n",
    "                    file_data['count'] = 1\n",
    "                    file_data['chip_id'] = filename.split('.')[0]\n",
    "                    file_data['date'] = filename.split('.')[1]\n",
    "                    \n",
    "                    # Extract session if it exists in filename\n",
    "                    split_filename = filename.split('.')\n",
    "                    if len(split_filename) >= 3:\n",
    "                        try:\n",
    "                            file_data['session'] = int(split_filename[2])\n",
    "                        except ValueError:\n",
    "                            print(f\"No valid session number found in {filename}. 'session' will be NaN for this file.\")\n",
    "                            file_data['session'] = np.nan\n",
    "                    else:\n",
    "                        file_data['session'] = np.nan\n",
    "\n",
    "                    if data.empty:\n",
    "                        data = file_data\n",
    "                    else:\n",
    "                        data = pd.concat([data, file_data], ignore_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def h5_to_pd(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Converts a H5 file to a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the file to convert\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The converted dataframe\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as file:\n",
    "        maps = pd.DataFrame(np.array(file['mapping']).tolist(), columns=['channel', 'electrode', 'x', 'y'])\n",
    "\n",
    "        ar = np.array(file['proc0']['spikeTimes'])\n",
    "        spikes = pd.DataFrame(ar.tolist(), columns=['frame', 'channel', 'amplitude'])\n",
    "\n",
    "        df = spikes.merge(maps, on='channel', how='left')\n",
    "        df['frame'] -= df['frame'].iloc[0]  # Normalizing frame values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c7af4",
   "metadata": {},
   "source": [
    "## Now some functions to get the spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6212db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_channels(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Group dataframe by 'channel' and return a dictionary.\"\"\"\n",
    "    channels = {}\n",
    "    for i, g in df.groupby('channel'):\n",
    "        channels.update({f'channel_{i}': g.reset_index(drop=True)})\n",
    "    return channels\n",
    "\n",
    "\n",
    "def process_channels(channels: Dict[str, pd.DataFrame]) -> Dict[str, pd.Series]:\n",
    "    \"\"\"Process channels dictionary and replace dataframes with 'frame' series.\"\"\"\n",
    "    processed_channels = {}\n",
    "    for channel_id, df in channels.items():\n",
    "        processed_channels[channel_id] = df['frame']\n",
    "    return processed_channels\n",
    "\n",
    "\n",
    "def extract_attributes(channels: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract attributes from the first channel's dataframe and return them in a dictionary.\"\"\"\n",
    "    first_channel_df = list(channels.values())[0]\n",
    "    return {\n",
    "        'chip_id': first_channel_df['chip_id'].unique()[0],\n",
    "        'session': first_channel_df['session'].unique()[0],\n",
    "        'date': first_channel_df['date'].unique()[0],\n",
    "    }\n",
    "\n",
    "\n",
    "def Get_channel_spiketimes(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract channel spike times from a dataframe.\"\"\"\n",
    "    # Early return if keys are not found\n",
    "    if not set(['session', 'chip_id', 'date']).issubset(dataframe.columns):\n",
    "        datas = group_by_channels(dataframe)\n",
    "        processed_datas = process_channels(datas)\n",
    "        return pd.DataFrame({'spike_times': [processed_datas]})\n",
    "\n",
    "    trials = {}\n",
    "    for kk, (i, g) in enumerate(dataframe.groupby(['session', 'chip_id', 'date'])):\n",
    "        trials.update({f'trials_{kk}': g.reset_index(drop=True)})\n",
    "\n",
    "    list_datas = []\n",
    "    for trial_df in trials.values():\n",
    "        list_datas.append(group_by_channels(trial_df))\n",
    "\n",
    "    new_list = [process_channels(datas) for datas in list_datas]\n",
    "\n",
    "    spike_times = []\n",
    "    chip_id = []\n",
    "    session = []\n",
    "    date = []\n",
    "\n",
    "    for datas in list_datas:\n",
    "        attributes = extract_attributes(datas)\n",
    "        chip_id.append(attributes['chip_id'])\n",
    "        session.append(attributes['session'])\n",
    "        date.append(attributes['date'])\n",
    "        spike_times.append(new_list.pop(0))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'chip_id': chip_id,\n",
    "        'session': session,\n",
    "        'date': date,\n",
    "        'spike_times': spike_times,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b86541",
   "metadata": {},
   "source": [
    "## And finally a helper function for statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada3bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_firing_stats(\n",
    "    spike_times: Dict[str, List[int]],\n",
    "    bin_size_sec: float,\n",
    "    total_exp_dur_sec: int,\n",
    "    sampling_freq: int,\n",
    "    experiment_len: int\n",
    ") -> Dict[str, List[float]]:\n",
    "    channels_firings = []\n",
    "    channels_firing_rates = []\n",
    "    channel_ISI = []\n",
    "    channel_ISI_mean = []\n",
    "    for channel, times in tqdm(spike_times.items(), desc = 'Computing stats...',\n",
    "                               total = len(spike_times.items())):\n",
    "        firing_temp = [0]*experiment_len\n",
    "        for item in times/bin_size_sec:\n",
    "            firing_temp[int(item)] += 1\n",
    "        channels_firings.append(firing_temp)\n",
    "        channels_firing_rates.append(np.array(firing_temp)/bin_size_sec)\n",
    "\n",
    "        temp = np.diff(times) / sampling_freq\n",
    "        channel_ISI.append(temp)\n",
    "        channel_ISI_mean.append(np.mean(temp))\n",
    "    \n",
    "    return {\n",
    "        'channels_firings': channels_firings,\n",
    "        'channels_firing_rates': channels_firing_rates,\n",
    "        'channel_ISI': channel_ISI,\n",
    "        'channel_ISI_mean': channel_ISI_mean,\n",
    "    }\n",
    "\n",
    "def firing_stats(\n",
    "    dataframe: pd.DataFrame,\n",
    "    bin_size_sec: float,\n",
    "    total_exp_dur_sec: int,\n",
    "    sampling_freq: int\n",
    ") -> pd.DataFrame:\n",
    "    experiment_len = int(total_exp_dur_sec * sampling_freq / bin_size_sec) + 1\n",
    "    spike_times_list = dataframe['spike_times'].apply(\n",
    "        lambda spike_times: {\n",
    "            channel: np.array(times)[np.array(times) < total_exp_dur_sec * sampling_freq]\n",
    "            for channel, times in spike_times.items()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    firing_stats_list = spike_times_list.apply(calculate_firing_stats, bin_size_sec=bin_size_sec,\n",
    "                                               total_exp_dur_sec=total_exp_dur_sec, sampling_freq=sampling_freq,\n",
    "                                               experiment_len=experiment_len)\n",
    "\n",
    "    dataframe['firing_counts/spikewords'] = [stats['channels_firings'] for stats in firing_stats_list]\n",
    "    dataframe['firing_rates'] = [stats['channels_firing_rates'] for stats in firing_stats_list]\n",
    "    dataframe['channel_mean_firing_rates'] = [np.mean(rates, axis=1) for rates in dataframe['firing_rates']]\n",
    "    dataframe['culture_mean_firing_rates'] = [np.mean(rates) for rates in dataframe['firing_rates']]\n",
    "    dataframe['channel_var_firing_rates'] = [np.var(rates, axis=1) for rates in dataframe['firing_rates']]\n",
    "    dataframe['culture_var_firing_rates'] = [np.var(rates) for rates in dataframe['firing_rates']]\n",
    "    dataframe['culture_max_firing_rates'] = [np.max(rates) for rates in dataframe['firing_rates']]\n",
    "    dataframe['channel_ISI'] = [stats['channel_ISI'] for stats in firing_stats_list]\n",
    "    dataframe['channel_ISI_mean'] = [stats['channel_ISI_mean'] for stats in firing_stats_list]\n",
    "    dataframe['culture_ISI_mean'] = [np.nanmean(mean) for mean in dataframe['channel_ISI_mean']]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3090a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firing_stats(dataframe, bin_size_sec, total_exp_dur_sec, sampling_freq):\n",
    "    \"\"\" \n",
    "    Calculate firing rate and other statistics for each channel.\n",
    "        \n",
    "    Args:\n",
    "        dataframe: dataframe containing spike times for each recording\n",
    "        bin_size_sec: length of time bins for calculating firing rate\n",
    "        total_exp_dur_sec: total length of recordings in seconds\n",
    "        sampling_freq: sampling frequency of the recordings in Hz\n",
    "\n",
    "    Returns:\n",
    "        dataframe: a copy of the input dataframe with additional columns for each calculated statistic\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get list of spike times dictionaries for each recording\n",
    "    spike_times_list = dataframe['spike_times']\n",
    "    \n",
    "    # For each spike times dictionary, remove spike times that exceed the total experiment duration\n",
    "    for spike_times in spike_times_list:    \n",
    "        for channel in spike_times.keys():\n",
    "            spike_times[channel] = spike_times[channel][spike_times[channel] < total_exp_dur_sec * sampling_freq]\n",
    "    \n",
    "    # Calculate number of bins for experiment\n",
    "    bin_size = sampling_freq*bin_size_sec\n",
    "    experiment_len = int(total_exp_dur_sec * sampling_freq/bin_size) + 1 \n",
    "\n",
    "    # Initialize lists to store results\n",
    "    all_firing_counts = []\n",
    "    all_firing_rates = []\n",
    "    channels_mean_firing = []\n",
    "    culture_mean_firing = []\n",
    "    culture_max_firing = []\n",
    "    channels_var_firing = []\n",
    "    culture_var_firing = []\n",
    "    all_channel_ISI = []\n",
    "    all_channel_ISI_means = []\n",
    "    culture_ISI_mean = []\n",
    "    \n",
    "    # For each spike times dictionary in the list\n",
    "    for spike_times in spike_times_list:\n",
    "        channels_firings = []\n",
    "        channels_firing_rates = []\n",
    "        channel_ISI = []\n",
    "        channel_ISI_mean = []\n",
    "        \n",
    "        # For each channel in the dictionary\n",
    "        for channel in spike_times.keys():\n",
    "            # Initialize list of firing counts for each bin\n",
    "            firing_counts = [0]*experiment_len\n",
    "            \n",
    "            # Increment firing count for each bin where a spike occurred\n",
    "            for spike_time in spike_times[channel]/bin_size:\n",
    "                firing_counts[int(spike_time)] += 1\n",
    "            \n",
    "            # Calculate firing rates for each bin\n",
    "            firing_rates = np.array(firing_counts) / bin_size_sec\n",
    "            \n",
    "            # Append firing counts and rates to lists\n",
    "            channels_firings.append(firing_counts)\n",
    "            channels_firing_rates.append(firing_rates)\n",
    "            \n",
    "            # Calculate interspike intervals and mean ISI\n",
    "            ISIs = np.diff(spike_times[channel]) / sampling_freq\n",
    "            channel_ISI.append(ISIs)\n",
    "            channel_ISI_mean.append(np.mean(ISIs))\n",
    "        \n",
    "        # Append results to lists\n",
    "        all_channel_ISI.append(channel_ISI)\n",
    "        all_channel_ISI_means.append(channel_ISI_mean)\n",
    "        culture_ISI_mean.append(np.nanmean(channel_ISI_mean))\n",
    "        all_firing_counts.append(channels_firings)\n",
    "        all_firing_rates.append(channels_firing_rates)\n",
    "        channels_mean_firing.append(np.mean(channels_firing_rates, axis=1))\n",
    "        culture_mean_firing.append(np.mean(channels_firing_rates))\n",
    "        channels_var_firing.append(np.var(channels_firing_rates, axis=1))\n",
    "        culture_var_firing.append(np.var(channels_firing_rates))\n",
    "        culture_max_firing.append(np.max(channels_firing_rates))\n",
    "\n",
    "    # Create a copy of the input dataframe and add new columns\n",
    "    dataframe_copy = dataframe.copy()\n",
    "    dataframe_copy['firing_counts/spikewords'] =  all_firing_counts\n",
    "    dataframe_copy['firing_rates'] =  all_firing_rates \n",
    "    dataframe_copy['channel_mean_firing_rates'] =  channels_mean_firing \n",
    "    dataframe_copy['culture_mean_firing_rates'] =  culture_mean_firing\n",
    "    dataframe_copy['channel_var_firing_rates'] =  channels_var_firing \n",
    "    dataframe_copy['culture_var_firing_rates'] =  culture_var_firing \n",
    "    dataframe_copy['culture_max_firing_rates'] =  culture_max_firing \n",
    "    dataframe_copy['channel_ISI'] =  all_channel_ISI \n",
    "    dataframe_copy['channel_ISI_mean'] =  all_channel_ISI_means \n",
    "    dataframe_copy['culture_ISI_mean'] =  culture_ISI_mean \n",
    "\n",
    "    return dataframe_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa89f7d",
   "metadata": {},
   "source": [
    "# Loading some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00f3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|█████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00, 11.94it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/cortical_labs_data/'\n",
    "\n",
    "data_test = read_maxwell_h5(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655e2589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>channel</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>electrode</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>count</th>\n",
       "      <th>chip_id</th>\n",
       "      <th>date</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>-245.276199</td>\n",
       "      <td>20386.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11614</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>-141.135452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11614</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>-327.031555</td>\n",
       "      <td>18454.0</td>\n",
       "      <td>3395.0</td>\n",
       "      <td>1452.5</td>\n",
       "      <td>1</td>\n",
       "      <td>11614</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>-319.245300</td>\n",
       "      <td>9410.0</td>\n",
       "      <td>2975.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11614</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>-393.214417</td>\n",
       "      <td>8550.0</td>\n",
       "      <td>3325.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11614</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258598</th>\n",
       "      <td>12002213</td>\n",
       "      <td>916</td>\n",
       "      <td>-7.162490</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9501</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258599</th>\n",
       "      <td>12002256</td>\n",
       "      <td>102</td>\n",
       "      <td>-6.378607</td>\n",
       "      <td>18283.0</td>\n",
       "      <td>402.5</td>\n",
       "      <td>1452.5</td>\n",
       "      <td>1</td>\n",
       "      <td>9501</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258600</th>\n",
       "      <td>12002407</td>\n",
       "      <td>1006</td>\n",
       "      <td>-8.997836</td>\n",
       "      <td>18997.0</td>\n",
       "      <td>1347.5</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9501</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258601</th>\n",
       "      <td>12002491</td>\n",
       "      <td>436</td>\n",
       "      <td>-9.532014</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9501</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258602</th>\n",
       "      <td>12002549</td>\n",
       "      <td>131</td>\n",
       "      <td>-7.961086</td>\n",
       "      <td>20401.0</td>\n",
       "      <td>2817.5</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9501</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258603 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            frame  channel   amplitude  electrode       x       y  count  \\\n",
       "0               0       11 -245.276199    20386.0  2555.0  1610.0      1   \n",
       "1               0       27 -141.135452        NaN     NaN     NaN      1   \n",
       "2               0       43 -327.031555    18454.0  3395.0  1452.5      1   \n",
       "3               0       53 -319.245300     9410.0  2975.0   735.0      1   \n",
       "4               0       69 -393.214417     8550.0  3325.0   665.0      1   \n",
       "...           ...      ...         ...        ...     ...     ...    ...   \n",
       "1258598  12002213      916   -7.162490     8470.0  1925.0   665.0      1   \n",
       "1258599  12002256      102   -6.378607    18283.0   402.5  1452.5      1   \n",
       "1258600  12002407     1006   -8.997836    18997.0  1347.5  1505.0      1   \n",
       "1258601  12002491      436   -9.532014     2238.0   665.0   175.0      1   \n",
       "1258602  12002549      131   -7.961086    20401.0  2817.5  1610.0      1   \n",
       "\n",
       "        chip_id        date  session  \n",
       "0         11614  2021-06-02        0  \n",
       "1         11614  2021-06-02        0  \n",
       "2         11614  2021-06-02        0  \n",
       "3         11614  2021-06-02        0  \n",
       "4         11614  2021-06-02        0  \n",
       "...         ...         ...      ...  \n",
       "1258598    9501  2021-06-01        3  \n",
       "1258599    9501  2021-06-01        3  \n",
       "1258600    9501  2021-06-01        3  \n",
       "1258601    9501  2021-06-01        3  \n",
       "1258602    9501  2021-06-01        3  \n",
       "\n",
       "[1258603 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aec0aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['frame', 'channel', 'amplitude', 'electrode', 'x', 'y', 'count',\n",
       "       'chip_id', 'date', 'session'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.columns\n",
    "# this would be environement frame, recording channel, voltage at the electrode, electrode number, x-y position on the dish \n",
    "# count ?? (seems useless as its a constant), experiment ID,\n",
    "# date of the experiment, session 1 and 3 are rest - 0 and 2 are game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e9aacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frame                 0\n",
       "channel              11\n",
       "amplitude   -245.276199\n",
       "electrode       20386.0\n",
       "x                2555.0\n",
       "y                1610.0\n",
       "count                 1\n",
       "chip_id           11614\n",
       "date         2021-06-02\n",
       "session               0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf52c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels are ranging from :\n",
      "0\n",
      "1023\n"
     ]
    }
   ],
   "source": [
    "print('Channels are ranging from :') \n",
    "print(np.min(sorted(data_test['channel'].unique())))\n",
    "print(np.max(sorted(data_test['channel'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa80c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electrodes are ranging from :\n",
      "2210.0\n",
      "23737.0\n"
     ]
    }
   ],
   "source": [
    "print('Electrodes are ranging from :') \n",
    "print(np.nanmin(sorted(data_test['electrode'].unique())))\n",
    "print(np.nanmax(sorted(data_test['electrode'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b17a5a",
   "metadata": {},
   "source": [
    "# Extracting spike times of each channel for each recorded culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Get_channel_spiketimes(data_test)\n",
    "\n",
    "data['session'] = data['session'].map(str)\n",
    "data['id'] = data[['chip_id', 'session', 'date']].agg('-'.join, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bcca03",
   "metadata": {},
   "source": [
    "# Computing a data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c218583",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"session\"] = pd.to_numeric(data[\"session\"])\n",
    "\n",
    "data_rest = data[data['session']%2 == 1]\n",
    "data_act = data[data['session']%2 == 0]\n",
    "data_act = data_act.reset_index()\n",
    "data_rest = data_rest.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3935d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fire_act = firing_stats(data_act,0.1,1200,20000)\n",
    "data_fire_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a428da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fire_rest = firing_stats(data_rest,0.1,600,20000)\n",
    "data_fire_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f2381",
   "metadata": {},
   "source": [
    "# Loading the event files \n",
    "### These are .txt files that contain information about the miss/hit of the balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Get_channel_spiketimes(data_test)\n",
    "\n",
    "data['session'] = data['session'].map(str)\n",
    "data['id'] = data[['chip_id', 'session', 'date']].agg('-'.join, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_chip_id = 11614\n",
    "chosen_chip_session = 0 # session 1 and 3 are rest - 0 and 2 are game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893de78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(directory, chip_id, session_id):\n",
    "    # Define the filename pattern\n",
    "    pattern = f'{chip_id}.*.{session_id}.events.txt'\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # If this file matches the pattern, return it\n",
    "        if fnmatch.fnmatch(filename, pattern):\n",
    "            return filename\n",
    "\n",
    "    # If no matching file was found, return None\n",
    "    return None\n",
    "\n",
    "filename = find_file('../data/cortical_labs_data/', str(chosen_chip_id), str(chosen_chip_session))\n",
    "if filename is None:\n",
    "    print('File not found.')\n",
    "else:\n",
    "    print('Found file:', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de67445",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open('../data/cortical_labs_data/%s' % filename, 'r') as file:\n",
    "    for line in file:\n",
    "        # Skip all 'info' lines except the stimulation mode\n",
    "        if line.startswith('info:'):\n",
    "            if 'stimulation mode' in line:\n",
    "                _, _, stim_mode = line.partition('stimulation mode:')\n",
    "                print(f'Stimulation mode: {stim_mode.strip()}')\n",
    "            continue\n",
    "        \n",
    "        # Skip 'motor layout' lines\n",
    "        if 'motor layout' in line:\n",
    "            continue\n",
    "\n",
    "        # Process event lines\n",
    "        timestamp, _, event = line.partition(':')\n",
    "        event_list.append({'timestamp': int(timestamp), 'event': event.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event_list[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
