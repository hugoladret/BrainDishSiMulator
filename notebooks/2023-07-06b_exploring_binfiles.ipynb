{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import functional as SF\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm \n",
    "import psweep as ps # <--\n",
    "\n",
    "import warnings # highly illegal move to make pandas compliant\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# We've also got binary replay files from cortical labs that could use some reverse engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_id = 9501 # experiment ID\n",
    "chip_session = 0 # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 10 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "len_trial = 100 # how long in bins is a trial (so in ms it's len_trial*binsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2ee02a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "timestamp out of range for platform time_t",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-06b_exploring_binfiles.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTimestamp: \u001b[39m\u001b[39m{\u001b[39;00mdt\u001b[39m}\u001b[39;00m\u001b[39m, Info: \u001b[39m\u001b[39m{\u001b[39;00minfo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Use the function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m explore_bin_file(data_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m9501.2021-06-01.0.replay.bin\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-06b_exploring_binfiles.ipynb Cell 5\u001b[0m in \u001b[0;36mexplore_bin_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m timestamp \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, data[:\u001b[39m8\u001b[39m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Convert timestamp to datetime (assuming it's a UNIX timestamp)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m dt \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mfromtimestamp(timestamp)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Interpret the last 2 bytes as a short integer (some kind of information)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-06b_exploring_binfiles.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m info \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m, data[\u001b[39m8\u001b[39m:])[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mOverflowError\u001b[0m: timestamp out of range for platform time_t"
     ]
    }
   ],
   "source": [
    "def explore_bin_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read(10)  # Read the first 10 bytes\n",
    "        hex_data = data.hex()  # Convert binary data to hexadecimal\n",
    "        print(hex_data)\n",
    "\n",
    "# Use the function\n",
    "explore_bin_file(data_path + '9501.2021-06-01.0.replay.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c01eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8c8b35",
   "metadata": {},
   "source": [
    "# Using our brand new dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61b42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels:  90%|████████▉ | 448/500 [00:00<00:00, 495.94it/s]"
     ]
    }
   ],
   "source": [
    "data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "# Find maximum time across all spike lists\n",
    "max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "# Create binned spikes tensor for each region\n",
    "sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "\n",
    "# Verifying that the tensor are binary files\n",
    "utils_tensor.check_binary(sensory_spikes_binned, \"sensory_spikes_binned\")\n",
    "utils_tensor.check_binary(up1_spikes_binned, \"up1_spikes_binned\")\n",
    "utils_tensor.check_binary(down1_spikes_binned, \"down1_spikes_binned\")\n",
    "utils_tensor.check_binary(up2_spikes_binned, \"up2_spikes_binned\")\n",
    "utils_tensor.check_binary(down2_spikes_binned, \"down2_spikes_binned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# A bit of preprocessing to get a nice PyTorch friendly format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing events\n",
    "# Process everything so its nice and milliseconds\n",
    "events[0]['event'] = 'motor layout: 0' # change the being game to motor layout for convenience \n",
    "for event in events:\n",
    "    event['norm_timestamp'] /= fs  # fs to seconds \n",
    "    event['norm_timestamp'] *= 1000  # seconds to ms\n",
    "    \n",
    "event_types = ['ball missed', 'ball bounce', 'ball return', 'motor layout: 0'] # these are all the labels\n",
    "labels = torch.tensor(utils_tensor.events_to_bins(events, event_types, 10, max_time_ms))\n",
    "assert labels.shape[-1] == sensory_spikes_binned.shape[-1] # make sure the labels and the data are the same length\n",
    "\n",
    "transformed_data, transformed_labels = utils_tensor.transform_data(labels, sensory_spikes_binned, len_trial) # change dataformat\n",
    "assert transformed_data.shape[1] == transformed_labels.shape[0] # make sure the labels and the data have the same trials\n",
    "\n",
    "# Create Dataset\n",
    "dataset = utils_tensor.CustomDataset(transformed_data, transformed_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82758fbe",
   "metadata": {},
   "source": [
    "# Now we define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59782b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, beta=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(x.size(1)):\n",
    "            cur1 = self.fc1(x[:,step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dishnet(params) :\n",
    "    # Unpack the parameters from the dict \n",
    "    num_inputs = params['num_inputs']\n",
    "    num_hidden = params['num_hidden']\n",
    "    num_outputs = params['num_outputs']\n",
    "    beta = params['beta']\n",
    "    num_epochs = params['num_epochs']\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    dishnet = Net(num_inputs = num_inputs, num_hidden = num_hidden, num_outputs=num_outputs, beta = beta).to(device)\n",
    "\n",
    "    loss_fn = SF.ce_count_loss()\n",
    "    optimizer = torch.optim.Adam(dishnet.parameters(), lr=lr, betas=(0.9, 0.999)) # future Hugo : these are optimizer's beta, not the SNN's, dont be stupid\n",
    "\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch = iter(data_loader)\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            dishnet.train()\n",
    "            spk_rec, _ = dishnet(data)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter == 100:\n",
    "                break\n",
    "            \n",
    "    print('\\n')\n",
    "    return loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:23:10.267179728          32   0.9  0.001          10          50         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:23:19.848337889          32   0.9  0.001          10         100         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                               batch_size  beta     lr  num_epochs  num_hidden  num_inputs  num_outputs\n",
      "2023-07-06 20:23:30.511329889          32   0.9  0.001          10         150         500            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the lists of values for each parameter you want to sweep over\n",
    "num_inputs = ps.plist(\"num_inputs\", [transformed_data.shape[-1]])\n",
    "num_hidden = ps.plist(\"num_hidden\", [50, 100, 150])\n",
    "num_outputs = ps.plist(\"num_outputs\", [len(event_types)])\n",
    "beta = ps.plist(\"beta\", [0.9])\n",
    "num_epochs = ps.plist(\"num_epochs\", [50])\n",
    "lr = ps.plist(\"lr\", [1e-3])\n",
    "batch_size = ps.plist(\"batch_size\", [32])\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = ps.pgrid((num_inputs, num_hidden, num_outputs, beta, num_epochs, lr, batch_size))\n",
    "\n",
    "# Define a function to run one instance of the experiment\n",
    "def run_experiment(params):\n",
    "    return {'loss': train_dishnet(params)}\n",
    "\n",
    "# Run the parameter sweep\n",
    "results = ps.run_local(run_experiment, param_grid, verbose = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
