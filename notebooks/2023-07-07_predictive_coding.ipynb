{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor, utils_pcn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# The paper says the dish is a predictive network, we are going to be more ambitious and actually prove it is one \n",
    "### But we'll start with MNIST first because we're not THAT ambitious"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddf274cc",
   "metadata": {},
   "source": [
    "## See [this paper](https://pubmed.ncbi.nlm.nih.gov/28333583/) for the maths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68d278a6",
   "metadata": {},
   "source": [
    "### We define the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fully connected (dense) layer class\n",
    "class FCLayer(object):\n",
    "    # -- INIT -- #\n",
    "    # Initialize the fully connected layer with input size, output size, activation function, and its derivative\n",
    "    def __init__(self, input_size, output_size,\n",
    "                f, df, device, dtype = torch.float64):\n",
    "        self.input_size = input_size  # layer input size\n",
    "        self.output_size = output_size  # layer output size\n",
    "        self.f = f  # activation function\n",
    "        self.df = df  # derivative of activation function\n",
    "        self.device = device  # cuda or cpu\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Weights initialization, as normal random variables\n",
    "        self.weights = torch.empty([self.input_size, self.output_size], dtype = self.dtype).normal_(mean=0.0, std=0.05).to(self.device)\n",
    "\n",
    "    # -- NETWORK ACTIVATION -- #\n",
    "    # Forward activation pass, multiplying an input x by the weights and passing them through the activation function\n",
    "    # Note that this makes the layer object keep x as .inp and x*weights as .activations\n",
    "    def forward(self, x):\n",
    "        self.inp = x.clone()\n",
    "        self.activations = torch.matmul(self.inp, self.weights)\n",
    "        return self.f(self.activations)\n",
    "\n",
    "    # Derivative backward activation pass, multiplying the input by the activation derivation and weight matrix\n",
    "    def backward(self, x):\n",
    "        self.fn_deriv = self.df(self.activations)\n",
    "        out = torch.matmul(x * self.fn_deriv, self.weights.T)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3ddf59f",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6376385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCNet_Bogacz(object):\n",
    "    def __init__(self, layers, n_inferences_steps, mu_lr,\n",
    "                theta_lr, batch_size,\n",
    "                fixed_predictions, update_weights_flag,\n",
    "                weight_clamp, pi_lr, pi_clamp,\n",
    "                mu_clamp, do_pi, device,\n",
    "                dtype = torch.float64):\n",
    "\n",
    "        self.layers = layers  # list of layers in the network\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.n_inferences_steps = n_inferences_steps  # number of steps in inference\n",
    "\n",
    "        self.fixed_predictions = fixed_predictions  # flags: update predictions\n",
    "        self.update_weights_flag = update_weights_flag  # flags: update weights\n",
    "        self.do_pi = do_pi  # flags: compute precision after the torch.eye init\n",
    "\n",
    "        self.mu_lr = mu_lr  # mu's shift rate\n",
    "        self.mu_clamp = mu_clamp  # clamp of mu values\n",
    "        \n",
    "        self.theta_lr = theta_lr  # theta learning rate\n",
    "        self.weight_clamp = weight_clamp  # clamp of theta value\n",
    "        \n",
    "        self.pi_lr = pi_lr  # pi learning rate\n",
    "        self.pi_clamp = pi_clamp  # clamp of pi value\n",
    "\n",
    "        self.device = device  # cuda or cpu\n",
    "        self.dtype = dtype\n",
    "        self.inited_pi = False  # switches off after precision init\n",
    "\n",
    "        # Initialize the lists of values\n",
    "        self.L = len(self.layers)\n",
    "        self.outs = [[] for i in range(self.L + 1)]  # forward activations\n",
    "        self.prediction_errors = [[] for i in range(self.L + 1)]  # prediction errors\n",
    "        self.predictions = [[] for i in range(self.L + 1)]  # predictions\n",
    "        self.mus = [[] for i in range(self.L + 1)]  # value neurons\n",
    "        \n",
    "        self.inhs = [[] for i in range(self.L + 1)]  # inhibitory neurons \n",
    "        self.inh_ws = [[] for i in range(self.L + 1)]  # inhibitor weights\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.inh_ws[i] = torch.eye(self.layers[i-1].output_size, dtype=torch.float64, device=device).unsqueeze(0).repeat(self.batch_size, 1, 1)\n",
    "        \n",
    "    # This is the meta function\n",
    "    @torch.no_grad()\n",
    "    def run_pc(self, inp, label):\n",
    "        # Set the initial value neurons for input and label\n",
    "        inp = inp.to(self.dtype)\n",
    "        self.mus[0] = inp.clone()\n",
    "        for i in range(1, self.L):\n",
    "            self.mus[i] = self.layers[i - 1].forward(self.mus[i - 1])\n",
    "        self.mus[-1] = label.clone()\n",
    "\n",
    "        # And propagate them through the network\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.outs[i] = self.layers[i-1].forward(self.mus[i-1])     \n",
    "            self.prediction_errors[i] = torch.zeros_like(self.outs[i], device = self.device, dtype = self.dtype)\n",
    "            self.inhs[i] = torch.zeros_like(self.prediction_errors[i], device = self.device, dtype = self.dtype)\n",
    "            \n",
    "        # Then perform the inference steps to reduce prediction errors\n",
    "        for _ in range(self.n_inferences_steps):\n",
    "            for j in range(1, self.L):\n",
    "                self.predictions[j] = self.layers[j].df(self.layers[j].activations)\n",
    "                dx_l = - self.prediction_errors[j] + torch.matmul(self.prediction_errors[j+1] * self.predictions[j], self.layers[j].weights.T)\n",
    "                self.mus[j] += self.mu_lr * torch.clamp(dx_l, -self.mu_clamp, self.mu_clamp)\n",
    "                \n",
    "            # Update once again the prediction errors - and the prediction, if they are not fixed\n",
    "            for i in range(1, self.L + 1):\n",
    "                self.prediction_errors[i] += self.mus[i] - self.outs[i]\n",
    "                self.prediction_errors[i] -= self.inhs[i]\n",
    "                self.inhs[i] += torch.bmm(self.inh_ws[i], self.prediction_errors[i].unsqueeze(2)).squeeze(2) - self.inhs[i]\n",
    "                \n",
    "                if not self.fixed_predictions:\n",
    "                    self.outs[i] = self.layers[i - 1].forward(self.mus[i - 1])\n",
    "\n",
    "        # Then do the slow weight updates\n",
    "        for (i, l) in enumerate(self.layers):\n",
    "            layer_term = self.prediction_errors[i + 1] * l.df(l.activations)\n",
    "            dw = torch.matmul(l.inp.T, layer_term)\n",
    "            l.weights += self.theta_lr * torch.clamp(dw, -self.weight_clamp, self.weight_clamp)\n",
    "\n",
    "        # and inhibitory weights updates\n",
    "        if self.do_pi:\n",
    "            for i in range(1, self.L + 1):\n",
    "                d_inh_w = torch.bmm(self.prediction_errors[i].unsqueeze(2), self.inhs[i].unsqueeze(1)).squeeze(0)\n",
    "                d_inh_w -= torch.eye(self.prediction_errors[i].shape[1], device=self.device, dtype = self.dtype).unsqueeze(0).repeat(self.batch_size, 1, 1)\n",
    "                self.inh_ws[i] += self.pi_lr * torch.clamp(d_inh_w, -self.pi_clamp, self.pi_clamp)\n",
    "\n",
    "        # Calculate the Free Energy (F) --> see Millidge's review equation 11\n",
    "        squared_L = 0\n",
    "        for i in range(1, self.L + 1):\n",
    "            # covars = torch.cov(self.prediction_errors[i].T + 1e-6)\n",
    "            # l = torch.sum(torch.matmul(covars.inverse(),self.prediction_errors[i].unsqueeze(2)**2).squeeze(2))\n",
    "            # l += torch.log(2*torch.pi*torch.sum(covars))\n",
    "            l = torch.sum(self.prediction_errors[i]**2)\n",
    "            squared_L += l\n",
    "\n",
    "        # Compute the accuracy of the model\n",
    "        acc = utils_pcn.accuracy(nograd_forward(self, inp), label)\n",
    "\n",
    "        # Return the Free Energy and accuracy\n",
    "        return squared_L.cpu().item(), acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a5f81ae",
   "metadata": {},
   "source": [
    "### And some utils to save us time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9cec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function to run everything\n",
    "def train(model, trainset, testset, n_epochs):\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accs = []\n",
    "        test_accs = []\n",
    "        for epoch in tqdm(range(n_epochs), total = n_epochs, desc = 'Training ...', position = 0, leave = True):\n",
    "            losslist, acclist = [], []\n",
    "            for i, (inp, label) in enumerate(trainset):\n",
    "                oh_label = utils_pcn.onehot(label, model.device).to(model.device)\n",
    "                squared_L, acc = model.run_pc(inp.to(model.device), oh_label)\n",
    "                losslist.append(squared_L)\n",
    "                acclist.append(acc)\n",
    "                \n",
    "            losses.append(np.mean(np.array(losslist)))\n",
    "            accs.append(np.mean(np.array(acclist)))\n",
    "\n",
    "            #print('Avg loss on epoch %s' % self.losses[-1])\n",
    "            #print('Avg acc on epoch %s' % self.accs[-1])\n",
    "\n",
    "            mean_test_acc, _ = test_accuracy(model, testset)\n",
    "            test_accs.append(mean_test_acc)\n",
    "            # self.save_model(model = model, precision_type = precision_type)\n",
    "    return losses, accs, test_accs\n",
    "\n",
    "# Forward through the network without gradient computation\n",
    "def nograd_forward(model, x):\n",
    "    with torch.no_grad():\n",
    "        for i, l in enumerate(model.layers):\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, precision_type):\n",
    "    for i, l in enumerate(model.layers):\n",
    "        np.save('./data/weights/layer_%s' % i, l.weights.cpu().detach().numpy())\n",
    "        if precision_type == 'regular' :\n",
    "            np.save('./data/precisions/layer_%s' % i, model.pis[i].cpu().detach().numpy())\n",
    "        else :\n",
    "            print('Precision type %s not implemented, not saving precision matrices' % precision_type)\n",
    "\n",
    "# Compute test accuracy\n",
    "def test_accuracy(model, testset):\n",
    "    accs = []\n",
    "    for i, (inp, label) in enumerate(testset):\n",
    "        pred_y = nograd_forward(model, inp.to(model.device, dtype = model.dtype))\n",
    "        acc = utils_pcn.accuracy(pred_y, utils_pcn.onehot(label, model.device).to(model.device))\n",
    "        accs.append(acc)\n",
    "    return np.mean(np.array(accs)), accs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8c8b35",
   "metadata": {},
   "source": [
    "# Now the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61b42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "batch_size = 64  # Batch size\n",
    "norm_factor = 1  # Normalization factor\n",
    "\n",
    "# Learning parameters\n",
    "epochs = 30  # iterations of the full inference steps\n",
    "n_inferences_steps = 500  # number of inference steps per input\n",
    "\n",
    "# Theta (weights) parameters\n",
    "theta_lr = 0.005  # learning rate for the weights theta\n",
    "weight_clamp = 50  # weight clamp\n",
    "\n",
    "# Mu (values) parameters\n",
    "mu_lr = 0.02  # shift rate for the value neurons\n",
    "mu_clamp = 1000  # value neuron clamp\n",
    "\n",
    "# Pi (precision) parameters\n",
    "pi_lr = 5e-4  # learning rate for the precision pi\n",
    "pi_clamp = 0.05  # precision clamp\n",
    "\n",
    "# Layer parameter\n",
    "update_weights_flag = True  # whether to learn the FF weights\n",
    "f = utils_pcn.tanh\n",
    "df = utils_pcn.tanh_deriv\n",
    "\n",
    "# Network parameter\n",
    "L1_size = 300\n",
    "L2_size = 100\n",
    "L3_size = 100\n",
    "fixed_predictions = True  # change the predictions or not\n",
    "\n",
    "\n",
    "# Torch parameters \n",
    "num_workers = 16\n",
    "pin_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af287a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# And off we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0  :  784  ->  300\n",
      "Layer 1  :  300  ->  100\n",
      "Layer 2  :  100  ->  100\n",
      "Layer 3  :  100  ->  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...:   0%|          | 0/30 [19:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-07_predictive_coding.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model \u001b[39m=\u001b[39m PCNet_Bogacz(layers \u001b[39m=\u001b[39m layers, batch_size \u001b[39m=\u001b[39m batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                         n_inferences_steps \u001b[39m=\u001b[39m n_inferences_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                         mu_lr \u001b[39m=\u001b[39m mu_lr, theta_lr \u001b[39m=\u001b[39m theta_lr, pi_lr \u001b[39m=\u001b[39m pi_lr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                         fixed_predictions \u001b[39m=\u001b[39m fixed_predictions, update_weights_flag\u001b[39m=\u001b[39mupdate_weights_flag,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                         weight_clamp \u001b[39m=\u001b[39m weight_clamp, mu_clamp \u001b[39m=\u001b[39m mu_clamp,  pi_clamp \u001b[39m=\u001b[39m pi_clamp,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                         device \u001b[39m=\u001b[39m device, do_pi \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# GPU burning\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m losses, accs, test_accs \u001b[39m=\u001b[39m train(model \u001b[39m=\u001b[39;49m model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                                 trainset \u001b[39m=\u001b[39;49m trainset[\u001b[39m0\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], testset \u001b[39m=\u001b[39;49m testset[\u001b[39m0\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                                 n_epochs \u001b[39m=\u001b[39;49m epochs)\n",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-07_predictive_coding.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainset, testset, n_epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (inp, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainset):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     oh_label \u001b[39m=\u001b[39m utils_pcn\u001b[39m.\u001b[39monehot(label, model\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     squared_L, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_pc(inp\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice), oh_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     losslist\u001b[39m.\u001b[39mappend(squared_L)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     acclist\u001b[39m.\u001b[39mappend(acc)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-07_predictive_coding.ipynb Cell 15\u001b[0m in \u001b[0;36mPCNet_Bogacz.run_pc\u001b[1;34m(self, inp, label)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_errors[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus[i] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mouts[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_errors[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minh_ws[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_errors[i]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m2\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_predictions:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-07_predictive_coding.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mouts[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mforward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generating datasets\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data/mnist_data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers = num_workers, pin_memory = pin_memory)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data/mnist_data', train=False,\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers = num_workers, pin_memory = pin_memory)\n",
    "\n",
    "testset = list(iter(testloader)) # this is just because I did a terrible implementation of PCNet\n",
    "trainset = list(iter(trainloader))\n",
    "\n",
    "for i, (img, label) in enumerate(trainset[:-2]) :\n",
    "    trainset[i] = (torch.Tensor(img).reshape(len(img), 784).to(torch.float64), label)\n",
    "    \n",
    "for i, (img, label) in enumerate(testset[:-2]) :\n",
    "    testset[i] = (torch.Tensor(img).reshape(len(img), 784).to(torch.float64), label)\n",
    "\n",
    "# Building the network\n",
    "shapes = [trainset[0][0].shape[1], L1_size, L2_size, L3_size, 10]\n",
    "layers = []\n",
    "for i in range(len(shapes)-1) :\n",
    "    print('Layer', i, ' : ', shapes[i], ' -> ', shapes[i+1])\n",
    "    layers.append(FCLayer(input_size = shapes[i],\n",
    "                            output_size = shapes[i+1], f = f, df = df,\n",
    "                            device = device))\n",
    "\n",
    "model = PCNet_Bogacz(layers = layers, batch_size = batch_size,\n",
    "                        n_inferences_steps = n_inferences_steps,\n",
    "                        mu_lr = mu_lr, theta_lr = theta_lr, pi_lr = pi_lr,\n",
    "                        fixed_predictions = fixed_predictions, update_weights_flag=update_weights_flag,\n",
    "                        weight_clamp = weight_clamp, mu_clamp = mu_clamp,  pi_clamp = pi_clamp,\n",
    "                        device = device, do_pi = True)\n",
    "\n",
    "# GPU burning\n",
    "losses, accs, test_accs = train(model = model, \n",
    "                                trainset = trainset[0:-2], testset = testset[0:-2],\n",
    "                                n_epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Plotting ------------------- #\n",
    "fig, axs = plt.subplots(figsize = (12,6), ncols = 2)\n",
    "\n",
    "axs[0].plot(losses[0], c = 'k', label = 'Loss')\n",
    "axs[0].set_title('Losses')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(accs[0], c = 'b', label = 'Train accuracy')\n",
    "axs[1].plot(test_accs[1], c = 'r', label = 'Test accuracy')\n",
    "axs[1].set_title('Accuracies')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "for ax in axs :\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
